\part*{Executive Summary}
\addcontentsline{toc}{part}{Executive Summary}
Summarize the objectives and key results of the deliverable.
PRACE deliverables document the work performed according to the Description of Action (Annex I to the Grant Agreement). All deliverables follow a consistent structure and layout. This prototype document describes in detail the conventions that the authors should adhere to. It is written as if it were a real deliverable. Delete the text in this deliverable bur retain the formatting.
The description of the work should be as concise as possible. It should also be explicit and state the results clearly. The external reviewers do not appreciate if they have to read ‘between the lines’. The combination of clear message, well written text – without spelling and grammatical errors – and a pleasing layout are as important for the acceptance of the project achievements by the reviewers and the EC as the actual work.
The Executive Summary should be less than one page. It should start on ‘Page 1’ and on the top page of the sheet.


\part{Introduction}
% Provide a brief description of the following (if appropriate):
% Objectives of the work related to the project as a whole;
% Purpose of the document;
% Structure of the document (what is in the different sections/chapters);
% Intended audience.


\part{Benchmark suite description}
%Description des codes/cas de tests/intérêt pour la communauté scientifique
This part will cover each code, presenting the interest for the scientific comunity as well as the test cases defined for the benchmarks.
As an extention to the EUABS, \ref{} most of codes presented in this suite are included in the later. Exceptions are PFARM which come from PRACE-2IP \ref{} and SHOC a synthetic benchmark suite.
\section{Alya}
Alya is a high performance computational mechanics code that can solve different coupled mechanics problems: incompressible/compressible flows, solid mechanics, chemistry, excitable media, heat transfer and Lagrangian particle transport. It is one single code. There are no particular parallel or individual platform versions. Modules, services and kernels can be compiled individually and used a la carte. The main discretisation technique employed in Alya is based on the variational multiscale finite element method to assemble the governing equations into Algebraic systems. These systems can be solved using solvers like GMRES, Deflated Conjugate Gradient, pipelined CG together with preconditioners like SSOR, Restricted Additive Schwarz, etc. The coupling between physics solved in different computational domains (like fluid-structure interactions) is carried out in a multi-code way, using different instances of the same executable. Asynchronous coupling can be achieved in the same way in order to transport Lagrangian particles.

\subsection{Code desctiption}
The code is parallelised with MPI and OpenMP. Two OpenMP strategies are available, without and with a colouring strategy to avoid ATOMICs during the assembly step. A CUDA version is also available for the different solvers. Alya has been also compiled for MIC (Intel Xeon Phi).

Alya is written in Fortran 1995 and the incompressible fluid module, present in the benchmark suite, is freely available. This module solves the Navier-Stokes equations using an Orthomin \ref{} method for the pressure Schur complement. This method is an algebraic split strategy which converges to the monolithic solution. At each linearisation step, the momentum is solved twice and the continuity equation is solved once or twice according to if the momentum preserving or the continuity preserving algorithm is selected.

\subsection{Test cases desctiption}
\subsubsection{Cavity-hexaedra elements (10M elements)}
This test is the classical lid-driven cavity. The problem geometry is a cube of dimensions 1x1x1. The fluid properties are density=1.0 and viscosity=0.01. Dirichlet boundary conditions are applied on all sides, with three no-slip walls and one moving wall with velocity equal to 1.0, which corresponds to a Reynolds number of 100. The Reynolds number is low so the regime is laminar and turbulence modelling is not necessary. The domain is discretised into 9800344 hexaedra elements. The solvers are the GMRES method for the momentum equations and the Deflated Conjugate Gradient to solve the continuity equation. This test case can be run using pure MPI parallelisation or the hybrid MPI/OpenMP strategy.

\subsubsection{Cavity-hexaedra elements (30M elements)}
This is the same cavity test as before but with 30M of elements. Note that a mesh multiplication strategy enables one to multiply the number of elements by powers of 8, by simply activating the corresponding option in the ker.dat file.

\subsubsection{Cavity-hexaedra elements-GPU version (10M elements)}
This is the same test as Test case 1, but using the pure MPI parallelisation strategy with acceleration of the algebraic solvers using GPUs.

\section{Code Saturne}
Code Saturne is an open-source CFD software package developed by EDF R\&D since 1997 and open-source since 2007. The Navier-Stokes equations are discretised following a finite volume method approach. The code can handle any type of mesh built with any type of cell/grid structure. Incompressible and compressible flows can be simulated, with or without heat transfer, and a range of turbulence models is available. The code can also be coupled with itself or other software to model some multiphysics problems (fluid-structure, fluid-conjugate heat transfer, for instance).

\subsection{Code desctiption}
Parallelism is handled by distributing the domain over the processors (several partitioning tools are available, either internally, i.e. SFC Hilbert and Morton, or through external libraryies, i.e. METIS Serial, ParMETIS, Scotch Serial, PT-SCOTCH. Communications between subdomains are performed through MPI. Hybrid parallelism using OpenMP has recently been optimised for improved multicore performance.

For incompressible simulations, most of the time is spent during the computation of the pressure through Poisson equations. PETSc and HYPRE have recently been linked to the code to offer alternatives to the internal solvers to compute the pressure. The developer’s version of PETSc supports CUDA and will be used in this benchmark suite.

Code Saturne is written in C, F95 and Python. It is freely available under the GPL license.

\subsection{Test cases desctiption}
Two test cases are dealt with, the former with a mesh made of tetrahedral cells and the latter with a mesh made of hexahedral cells. Both configurations are meant for incompressible laminar flows. Note that both configurations will also be used in the regular UEABS

\subsubsection{Flow in a 3-D lid-driven cavity (tetrahedral cells)}
The geometry is very simple, i.e. a cube, but the mesh is built using tetrahedral cells. The Reynolds number is set to 400, and symmetry boundary conditions are applied in the spanwise direction. The case is modular and the mesh size can easily been varied. The largest mesh has about 13 million cells.

This test case is expected to scale efficiently to 1000+ nodes.

\subsubsection{3-D Taylor-Green vortex flow (hexahedral cells)}
The Taylor-Green vortex flow is traditionally used to assess the accuracy of CFD code numerical schemes. Periodicity is used in the 3 directions. The total kinetic energy (integral of the velocity) and enstrophy (integral of the vorticity) evolutions as a function of the time are looked at. Code Saturne is set for 2nd order time and spatial schemes, and three meshes are considered, containing 1283, 2563 and 5123 cells, respectively.

This test case is expected to scale efficiently to 4000+ nodes for the largest mesh.

\section{CP2K}


\part{Targeted architechture}
Description des machines K40/P100/ Xeons Phi

\part{Applications performance}
Présentation des résultats
topo comparatif des deux architechture

\part*{Conclusion}
open vers le merge des suite de bench prace
